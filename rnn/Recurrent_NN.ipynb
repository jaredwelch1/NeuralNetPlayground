{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network \n",
    "\n",
    "This notebook is an exploration of recurrent nerual networks. They can be read about in detail [here](https://en.wikipedia.org/wiki/Recurrent_neural_network)\n",
    "\n",
    "The purpose of a RNN simply put is to apply deep learning to patterns over time. Using LSTM (long short term memory), a RNN is able to take in sequences of patterns and predict based on input patterns what the pattern would be if continued. A very easy visualization is text data. If given a sentence, \"I am running quickl\", a well trained RNN on text data would be able to tell you the next character probably should be 'y', making the sentence \"I am running quickly\". This trivial example can be expanded, and using a full trained RNN in this manner would in theory allow the RNN to generate text based on the patterns it has trained on.\n",
    "\n",
    "This notebook will attempt to create and train a Recurrent NN on shakespeare's written works, and then attempt to use that trained NN to generate original text that resembles Shakespeare. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import layers\n",
    "from tensorflow.contrib import rnn\n",
    "import glob\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some values to be used in model creation \n",
    "\n",
    "SEQLEN = 30 # length of seq of characters fed into rnn\n",
    "BATCHSIZE = 400 # seq / batch \n",
    "ALPHASIZE = 98 # number of possible values for a character (important for softmax layer and input sizing)\n",
    "INTERNALSIZE = 512 # size of GRU cell internally \n",
    "NLAYERS = 3 # how many are we stacking \n",
    "learning_rate = 0.001\n",
    "dropout_pkeep = 0.8 # probability of dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions for handling text data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specification of the supported alphabet (subset of ASCII-7)\n",
    "# 10 line feed LF\n",
    "# 32-64 numbers and punctuation\n",
    "# 65-90 upper-case letters\n",
    "# 91-97 more punctuation\n",
    "# 97-122 lower-case letters\n",
    "# 123-126 more punctuation\n",
    "def convert_from_alphabet(a):\n",
    "    \"\"\"Encode a character\n",
    "    :param a: one character\n",
    "    :return: the encoded value\n",
    "    \"\"\"\n",
    "    if a == 9:\n",
    "        return 1\n",
    "    if a == 10:\n",
    "        return 127 - 30  # LF\n",
    "    elif 32 <= a <= 126:\n",
    "        return a - 30\n",
    "    else:\n",
    "        return 0 # unknown\n",
    "\n",
    "# encoded values:\n",
    "# unknown = 0\n",
    "# tab = 1\n",
    "# space = 2\n",
    "# all chars from 32 to 126 = c-30\n",
    "# LF mapped to 127-30\n",
    "def convert_to_alphabet(c, avoid_tab_and_lf=False):\n",
    "    \"\"\"Decode a code point\n",
    "    :param c: code point\n",
    "    :param avoid_tab_and_lf: if True, tab and line feed characters are replaced by '\\'\n",
    "    :return: decoded character\n",
    "    \"\"\"\n",
    "    if c == 1:\n",
    "        return 32 if avoid_tab_and_lf else 9  # space instead of TAB\n",
    "    if c == 127 - 30:\n",
    "        return 92 if avoid_tab_and_lf else 10  # \\ instead of LF\n",
    "    if 32 <= c + 30 <= 126:\n",
    "        return c + 30\n",
    "    else:\n",
    "        return 0  \n",
    "    \n",
    "# helper function for encoding data\n",
    "def encode_text(s):\n",
    "    \"\"\"Encode a string.\n",
    "    :param s: a text string\n",
    "    :return: encoded list of code points\n",
    "    \"\"\"\n",
    "    return list(map(lambda a: convert_from_alphabet(ord(a)), s))\n",
    "\n",
    "\n",
    "def read_data_files(directory, validation=True):\n",
    "        \"\"\"read data files based on glob given\n",
    "        :param directory: glob for data directory (example \"shakespeare/*.txt\")\n",
    "        :param validation: if True, sets last file aside as validation data\n",
    "        :return training_text, validation text, list of loaded file names\"\"\"\n",
    "        train_txt = []\n",
    "        bookranges = []\n",
    "        file_list = glob.glob(directory, recursive=True)\n",
    "        for f in file_list:\n",
    "            with open(f, \"r\") as file_txt:\n",
    "                print(\"Loading file \" + f)\n",
    "                start = len(train_txt)\n",
    "                train_txt.extend(encode_text(file_txt.read()))\n",
    "                end = len(train_txt)\n",
    "                # give us name of file and where it is in the text array\n",
    "                bookranges.append({\"start\": start, \"end\": end, \"name\": f.rsplit(\"/\", 1)[-1]})\n",
    "        if len(bookranges) == 0:\n",
    "            sys.exit('No training data has been found. Aborting')\n",
    "        \n",
    "        # For validation, use roughly 90K of text,\n",
    "        # but no more than 10% of the entire text\n",
    "        # and no more than 1 book in 5 => no validation at all for 5 files or fewer.\n",
    "\n",
    "        # 10% of the text is how many files ?\n",
    "        total_len = len(train_txt)\n",
    "        validation_len = 0\n",
    "        nb_books1 = 0\n",
    "        for book in reversed(bookranges):\n",
    "            validation_len += book[\"end\"]-book[\"start\"]\n",
    "            nb_books1 += 1\n",
    "            if validation_len > total_len // 10:\n",
    "                break\n",
    "\n",
    "        # 90K of text is how many books ?\n",
    "        validation_len = 0\n",
    "        nb_books2 = 0\n",
    "        for book in reversed(bookranges):\n",
    "            validation_len += book[\"end\"]-book[\"start\"]\n",
    "            nb_books2 += 1\n",
    "            if validation_len > 90*1024:\n",
    "                break\n",
    "\n",
    "        # 20% of the books is how many books ?\n",
    "        nb_books3 = len(bookranges) // 5\n",
    "\n",
    "        # pick the smallest\n",
    "        nb_books = min(nb_books1, nb_books2, nb_books3)\n",
    "\n",
    "        if nb_books == 0 or not validation:\n",
    "            cutoff = len(train_txt)\n",
    "        else:\n",
    "            cutoff = bookranges[-nb_books][\"start\"]\n",
    "        valitext = train_txt[cutoff:]\n",
    "        training_text = train_txt[:cutoff]\n",
    "        return training_text, valitext, bookranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# location of training/test text \n",
    "data_dir_glob = \"shakespeare/*.txt\"\n",
    "\n",
    "# train_text and vali_text are lists of characters, encoded to numeric values to be fed to the rnn in batches\n",
    "train_text, vali_text, file_names = read_data_files(data_dir_glob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle properly batching the data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_size = len(train_text) // (BATCHSIZE * SEQLEN)\n",
    "\n",
    "# Need a special helper function to give a generator object that yields our batches to us\n",
    "# This will ensure that the batches are properly setup so that sequences continue between batches\n",
    "\n",
    "# example:\n",
    "#     Batch 1 [ The cow jumped ove] Batch 2 [r the moon]\n",
    "# \n",
    "# If we do not maintain the ordering of sequences in this manner, training is not as effective \n",
    "def rnn_minibatching(raw_data, batch_size, sequence_size, nb_epochs):\n",
    "    data = np.array(raw_data)\n",
    "    data_len = data.shape[0]\n",
    "    # using (data_len - 1) so that we can make sure the y+1 sequence is also handled\n",
    "    nb_batches = (data_len - 1) // (batch_size * sequence_size)\n",
    "    assert nb_batches > 0, \"Not enough data for a single batch\"\n",
    "    rounded_data_len = int(nb_batches * batch_size * sequence_size)\n",
    "    xdata = np.reshape(data[0:rounded_data_len], [batch_size, nb_batches * sequence_size])\n",
    "    ydata = np.reshape(data[1:rounded_data_len+1], [batch_size, nb_batches * sequence_size])\n",
    "    \n",
    "    for epoch in range(nb_epochs):\n",
    "        for batch in range(nb_batches):\n",
    "            x = xdata[:, batch * sequence_size:(batch + 1) * sequence_size]\n",
    "            y = ydata[:, batch * sequence_size:(batch + 1) * sequence_size]\n",
    "            x = np.roll(x, -epoch, axis=0)\n",
    "            y = np.roll(y, -epoch, axis=0)\n",
    "            yield x, y, epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition \n",
    "\n",
    "The RNN structure we want will be 3 GRU cells deep, unrolled over sequences of 30 characters. This means that our RNN takes in sequenecs of 30 characters, and has 3 layers. Internally, GRU cells are gated cells have internal sizes of N. In this case, an internal size of 512 was chosen. \n",
    "\n",
    "After being fed through the RNN, a softmax output layer will be used to predict the correct character to be appended to the current sequence. This softmax will be shaped as [BATCHSIZE x SEQLEN, ALPHASIZE], to read in batches of sequences and output softmax values for the whole batch. \n",
    "\n",
    "Feeding in our data will be batches of sequences, and those sequences are all characters. Based on that, input data will be a 3d tensor of size [Batch size, sequence length, alpha size] (where alpha size is the range of possible ASCII values for the characters). The reason for this is to one-hot encode our input data based on ALPHA SIZE. \n",
    "\n",
    "For softmax readout, a trick has been implemented to make training easier. Since our RNN output will be in the shape \n",
    "[ BATCHSIZE, SEQLEN, INTERNALSIZE ], in order to do a softmax readout, we will convert to [ BATCHSIZE * SEQLEN, INTERNALSIZE], then convert that to [BATCHSIZE * SEQLEN, ALPHASIZE ]. This way, we can train quickly on batches, taking advantage of tensor shaping to predict softmax values for the entire batch of sequences at once, and readout the softmax for each sequence in the batch to predict the next character in the sequence.\n",
    "\n",
    "[ batch of sequences ] -> one-hot -> RNN with dropout -> softmax on batch of sequences -> character predict for each sequence in the batch\n",
    "\n",
    "Validation consists of reading a sequence, predicting the resulting sequence, and comparing the resulting sequence to the original sequence, offset by one character. \n",
    "\n",
    "    ex:\n",
    "        input: \"I am a cool gu\"\n",
    "        valid output: \" am a cool guy\"\n",
    "\n",
    "From this, X will be our sequences, and Y_ will be those sequences, shifted by 1 character. \n",
    "\n",
    "Hin is the original input state, since RNN are basically state machines we need an initial state to begin.\n",
    "\n",
    "Finally, while training, it is important to note that each output from the previous training step will become the new input for the following training step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "lr = tf.placeholder(tf.float32, name='lr')\n",
    "pkeep = tf.placeholder(tf.float32, name='pkeep')\n",
    "batchsize = tf.placeholder(tf.int32, name='batchsize')\n",
    "\n",
    "# inputs\n",
    "X = tf.placeholder(tf.uint8, [None, None], name='X') # will be [BATCHSIZE, SEQLEN]\n",
    "\n",
    "# one-hot encode our input taking it from 2d to 3d tensor\n",
    "Xo = tf.one_hot(X, ALPHASIZE, 1.0, 0.0)              # will be [BATCHSIZE, SEQLEN, ALPHASIZE]\n",
    "\n",
    "# do the same for output, which is same char seq shifted by 1 character\n",
    "Y_ = tf.placeholder(tf.uint8, [None, None], name=\"Y_\") # [BATCHSIZE, SEQLEN]\n",
    "Yo_ = tf.one_hot(Y_, ALPHASIZE, 1.0, 0.0)              # [BATCHSIZE, SEQLEN, ALPHASIZE]\n",
    "\n",
    "# Input state for our rnn (see documentation on RNN if not sure what this is compared to input data)\n",
    "Hin = tf.placeholder(tf.float32, [None, INTERNALSIZE*NLAYERS], name=\"Hin\") # [BATCHSIZE, INTERNALSIZE*NLAYERS]\n",
    "\n",
    "# NLAYERS=3 GRU cells unrolled over a SEQLEN of 30 chars\n",
    "# dynamic_rnn can infer SEQLEN from input size\n",
    "\n",
    "cells = [rnn.GRUCell(INTERNALSIZE) for _ in range(NLAYERS)]\n",
    "# dropout implementation\n",
    "dropcells = [rnn.DropoutWrapper(cell, input_keep_prob=pkeep) for cell in cells]\n",
    "multicell = rnn.MultiRNNCell(dropcells, state_is_tuple=False)\n",
    "multicell = rnn.DropoutWrapper(multicell, output_keep_prob=pkeep)\n",
    "\n",
    "Yr, H = tf.nn.dynamic_rnn(multicell, Xo, dtype=tf.float32, initial_state=Hin)\n",
    "# Yr: [ BATCHSIZE, SEQLEN, INTERNALSIZE]\n",
    "# H: [ BATCHSIZE, INTERNALSIZE*NLAYERS ]\n",
    "\n",
    "H = tf.identity(H, name='H') # give it a name\n",
    "\n",
    "# Softmax output layer \n",
    "# Flatten the first two dimensions of the output [ BATCHSIZE, SEQLEN, ALPHASIZE] => [BATCHSIZE * SEQLEN, ALPHASIZE]\n",
    "# then apply softmax readout\n",
    "\n",
    "Yflat = tf.reshape(Yr, [-1, INTERNALSIZE])   # [ BATCHSIZE x SEQLEN, INTERNALSIZE]\n",
    "Ylogits = layers.linear(Yflat, ALPHASIZE)    # [ BATCHSIZE X SEQLEN, ALPHASIZE]\n",
    "Yflat_ = tf.reshape(Yo_, [-1, ALPHASIZE])    # [ BATCHSIZE x SEQLEN, ALPHASIZE]\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=Ylogits, labels=Yflat_) # [ BATCHSIZE x SEQLEN ]\n",
    "loss = tf.reshape(loss, [batchsize, -1])     # [ BATCHSIZE, SEQLEN ]\n",
    "Yo = tf.nn.softmax(Ylogits, name=\"Yo\")       # [ BATCHSIZE x SEQLEN, ALPHASIZE ]\n",
    "Y = tf.argmax(Yo, 1)                         # [ BATCHSIZE x SEQLEN ]\n",
    "Y = tf.reshape(Y, [batchsize, -1], name=\"Y\") # [ BATCHSIZE, SEQLEN ]\n",
    "train_step = tf.train.AdamOptimizer(lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "istate = np.zeros([BATCHSIZE, INTERNALSIZE*NLAYERS]) # init state\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab some statistics stuff to show during training\n",
    "\n",
    "seqloss = tf.reduce_mean(loss, 1)\n",
    "batchloss = tf.reduce_mean(seqloss)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(Y_, tf.cast(Y, tf.uint8)), tf.float32))\n",
    "loss_summary = tf.summary.scalar(\"batch_loss\", batchloss)\n",
    "acc_summary = tf.summary.scalar(\"batch_accuracy\", accuracy)\n",
    "summaries = tf.summary.merge([loss_summary, acc_summary])\n",
    "\n",
    "# init some tensorboard stuff. Saves tensor into folder of log files\n",
    "timestamp = str(math.trunc(time.time()))\n",
    "summary_writer = tf.summary.FileWriter(\"log/\" + timestamp + \"-training\")\n",
    "validation_writer = tf.summary.FileWriter(\"log/\" + timestamp + \"-validation\")\n",
    "\n",
    "# this will be a place to save trained models\n",
    "if not os.path.exists(\"checkpoints\"):\n",
    "    os.mkdir(\"checkpoints\")\n",
    "saver = tf.train.Saver(max_to_keep=1000)\n",
    "\n",
    "# setup some stuff to handle displaying progress\n",
    "DISPLAY_FREQ = 50 # show every 100 batches\n",
    "_100_batches = DISPLAY_FREQ * SEQLEN * BATCHSIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training \n",
    "\n",
    "Now that model has been created and batching has been handlded by the helper function created earlier, grab batches of input and valid output in a loop and begin training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 10\n",
    "prev_epoch = -1\n",
    "epoch_start_time = datetime.now()\n",
    "for x, y_, epoch in rnn_minibatching(train_text, BATCHSIZE, SEQLEN, nb_epochs=NUM_EPOCHS):\n",
    "    if epoch != prev_epoch:\n",
    "        if prev_epoch != -1:\n",
    "            print(\"Training finished on epoch\" + str(prev_epoch))\n",
    "            curr_time = datetime.now()\n",
    "            epoch_time = curr_time - epoch_start_time\n",
    "            print(\"Time for training (Days:hours:seconds)\" + str(epoch_time))\n",
    "            epoch_start_time = curr_time\n",
    "            \n",
    "        print(\"Training starting on epoch \" + str(epoch))\n",
    "        prev_epoch = epoch\n",
    "    # train on a minibatch \n",
    "    feed_dict = {X: x, Y_: y_, Hin: istate, lr: learning_rate, pkeep: dropout_pkeep, batchsize: BATCHSIZE}\n",
    "    _, y, ostate, = sess.run([train_step, Y, H], feed_dict=feed_dict)\n",
    "    \n",
    "    # log training data for tensorboard\n",
    "    if step % _100_batches == 0:\n",
    "        feed_dict = {X: x, Y_: y_, Hin: istate, pkeep: 1.0, batchsize: BATCHSIZE}  # no dropout for validation\n",
    "        y, l, bl, acc, smm = sess.run([Y, seqloss, batchloss, accuracy, summaries], feed_dict=feed_dict)\n",
    "        summary_writer.add_summary(smm, step)\n",
    "    \n",
    "    # log a validation step \n",
    "    if step % _100_batches == 0 and len(vali_text) > 0:\n",
    "        VALI_SEQLEN = 1*1024 # Sequence length for validation \n",
    "        bsize = len(vali_text) // VALI_SEQLEN\n",
    "        vali_x, vali_y, _ = next(rnn_minibatching(vali_text, bsize, VALI_SEQLEN, 1)) # all data in a single batch\n",
    "        vali_nullstate = np.zeros([bsize, INTERNALSIZE*NLAYERS])\n",
    "        feed_dict = {X: vali_x, Y_: vali_y, Hin: vali_nullstate, pkeep: 1.0, batchsize: bsize}\n",
    "        ls, acc, smm = sess.run([batchloss, accuracy, summaries], feed_dict=feed_dict)\n",
    "        validation_writer.add_summary(smm, step)\n",
    "    \n",
    "    # save a checkpoint \n",
    "    if step // 10 % _100_batches == 0:\n",
    "        saved_file = saver.save(sess, 'checkpoints/rnn_train_' + timestamp, global_step=step)\n",
    "        print(\"Saved checkpoint file: \" + saved_file)\n",
    "    \n",
    "    istate = ostate\n",
    "    step += BATCHSIZE * SEQLEN\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
